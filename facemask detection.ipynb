{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils==0.5.3\n",
      "  Using cached https://files.pythonhosted.org/packages/b5/94/46dcae8c061e28be31bcaa55c560cb30ee9403c9a4bb2659768ec1b9eb7d/imutils-0.5.3.tar.gz\n",
      "Building wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py): started\n",
      "  Building wheel for imutils (setup.py): finished with status 'done'\n",
      "  Created wheel for imutils: filename=imutils-0.5.3-cp37-none-any.whl size=25853 sha256=f7ecedd5caf2e38055d7cd7a7a778d242d74e9d99b2d8163b1a3d372559c98c8\n",
      "  Stored in directory: C:\\Users\\Aravind\\AppData\\Local\\pip\\Cache\\wheels\\16\\84\\1f\\bf88641293cda2c8be81a5c4b8ca973dd9125a6dc3767417fd\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install imutils==0.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing the neccessary libraries \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intializing the global variabels\n",
    "EPOCH=20\n",
    "INIT_LR=1e-4\n",
    "bs=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the path of the dataset to the varibels \n",
    "dirc=r\"C:\\Users\\Aravind\\samples\"\n",
    "CATGS = [\"with_mask\", \"without_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aravind\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "labels=[]\n",
    "#intializing the two list variables to store the images and the corresponding labels in the image\n",
    "for category in CATGS:\n",
    "        path=os.path.join(dirc,category)#joining the each folders path with already existing path of the directory\n",
    "        for img in os.listdir(path):#travesrsing through the images in the list\n",
    "            img_path=os.path.join(path,img)\n",
    "            image=load_img(img_path,target_size=(224,224))#converting the image to uniform size(224,224)\n",
    "            image=img_to_array(image)#converting the image into array of pixels\n",
    "            image=preprocess_input(image)#preprocessing the array of pixel\n",
    "\n",
    "            data.append(image)#appending the processed array of the pixel in the list \n",
    "            labels.append(category)#appending the corresponding category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the categorical variable using the label binarizer\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels=to_categorical(labels)# to categorical used to convert the encoded vaues in the form of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting our list to np arrays because neural network procces only the array\n",
    "data=np.array(data,dtype=\"float32\")\n",
    "labels=np.array(labels)\n",
    "#separataing the data set for the trainning and testing\n",
    "trainx,testx,trainy,testy=train_test_split(data,labels,test_size=0.20,stratify=labels,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image agumentaion is done because we have around 1000 records for trainnig and testing\n",
    "aug=ImageDataGenerator(rotation_range=20,\n",
    "                       zoom_range=0.15,\n",
    "                       width_shift_range=0.2,\n",
    "                       height_shift_range=0.2,\n",
    "                       shear_range=0.15,\n",
    "                       horizontal_flip=True,\n",
    "                       fill_mode='nearest' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aravind\\Anaconda3\\lib\\site-packages\\keras_applications\\mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aravind\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#creating our base model using mobilenets\n",
    "baseModel=MobileNetV2(weights=\"imagenet\",include_top=False,#include_top used to represent the fully connected layer at the top. here the value is given false because we are doing that mannualy                     \n",
    "input_tensor=Input(shape=(224,224,3)))#intialzing the input dimension of the image\n",
    "headModel=baseModel.output#we are taking the output of the basemodel as the input\n",
    "headModel=AveragePooling2D(pool_size=(7,7))(headModel)\n",
    "#(headModel)using this we are connnecting the output of the headmodel to this layer\n",
    "headModel=Flatten(name=\"flatten\")(headModel)\n",
    "headModel=Dense(128,activation=\"relu\")(headModel)\n",
    "headModel=Dropout(0.5)(headModel)\n",
    "headModel=Dense(2,activation=\"softmax\")(headModel)\n",
    "\n",
    "\n",
    "model=Model(inputs=baseModel.input,outputs=headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in baseModel.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] compiling model..\n"
     ]
    }
   ],
   "source": [
    "print(\"[info] compiling model..\")\n",
    "opt=Adam(lr=INIT_LR,decay=INIT_LR/EPOCH)#intializing the adam optimizer for optimization of our model\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])#compiling our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(len(trainx)//bs)\n",
    "print(len(testx)//bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] traininf head...\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\Aravind\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "94/95 [============================>.] - ETA: 25s - loss: 0.5406 - acc: 0.7302Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 199s 260ms/sample - loss: 0.3808 - acc: 0.8540\n",
      "95/95 [==============================] - 2601s 27s/step - loss: 0.5387 - acc: 0.7317 - val_loss: 0.3911 - val_acc: 0.8540\n",
      "Epoch 2/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.2837 - acc: 0.8861 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 52s 68ms/sample - loss: 0.3517 - acc: 0.8540\n",
      "95/95 [==============================] - 864s 9s/step - loss: 0.2825 - acc: 0.8869 - val_loss: 0.3625 - val_acc: 0.8540\n",
      "Epoch 3/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.2328 - acc: 0.9127 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 55s 71ms/sample - loss: 0.3084 - acc: 0.8879\n",
      "95/95 [==============================] - 879s 9s/step - loss: 0.2327 - acc: 0.9127 - val_loss: 0.3010 - val_acc: 0.8879\n",
      "Epoch 4/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.2019 - acc: 0.9297 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 71s 93ms/sample - loss: 0.2552 - acc: 0.9126\n",
      "95/95 [==============================] - 885s 9s/step - loss: 0.2029 - acc: 0.9295 - val_loss: 0.2402 - val_acc: 0.9126\n",
      "Epoch 5/20\n",
      "94/95 [============================>.] - ETA: 4:49 - loss: 0.1478 - acc: 0.9507Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 73s 95ms/sample - loss: 0.3080 - acc: 0.8996\n",
      "95/95 [==============================] - 27308s 287s/step - loss: 0.1478 - acc: 0.9509 - val_loss: 0.2815 - val_acc: 0.8996\n",
      "Epoch 6/20\n",
      "94/95 [============================>.] - ETA: 11s - loss: 0.1504 - acc: 0.9480Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 74s 96ms/sample - loss: 0.2919 - acc: 0.9074\n",
      "95/95 [==============================] - 1119s 12s/step - loss: 0.1503 - acc: 0.9479 - val_loss: 0.2635 - val_acc: 0.9074\n",
      "Epoch 7/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.1634 - acc: 0.9367 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 51s 66ms/sample - loss: 0.2772 - acc: 0.9100\n",
      "95/95 [==============================] - 854s 9s/step - loss: 0.1622 - acc: 0.9374 - val_loss: 0.2453 - val_acc: 0.9100\n",
      "Epoch 8/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.1338 - acc: 0.9548 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 51s 67ms/sample - loss: 0.3331 - acc: 0.9009\n",
      "95/95 [==============================] - 828s 9s/step - loss: 0.1332 - acc: 0.9549 - val_loss: 0.2999 - val_acc: 0.9009\n",
      "Epoch 9/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.1248 - acc: 0.9519 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 51s 67ms/sample - loss: 0.4408 - acc: 0.8735\n",
      "95/95 [==============================] - 825s 9s/step - loss: 0.1240 - acc: 0.9524 - val_loss: 0.4084 - val_acc: 0.8735\n",
      "Epoch 10/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.1096 - acc: 0.9607 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 51s 66ms/sample - loss: 0.4958 - acc: 0.8657\n",
      "95/95 [==============================] - 824s 9s/step - loss: 0.1100 - acc: 0.9601 - val_loss: 0.4696 - val_acc: 0.8657\n",
      "Epoch 11/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.1052 - acc: 0.9610 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 52s 67ms/sample - loss: 0.3382 - acc: 0.9061\n",
      "95/95 [==============================] - 828s 9s/step - loss: 0.1063 - acc: 0.9608 - val_loss: 0.3011 - val_acc: 0.9061\n",
      "Epoch 12/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.1125 - acc: 0.9604 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 51s 67ms/sample - loss: 0.3738 - acc: 0.8970\n",
      "95/95 [==============================] - 825s 9s/step - loss: 0.1126 - acc: 0.9598 - val_loss: 0.3413 - val_acc: 0.8970\n",
      "Epoch 13/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.1123 - acc: 0.9564 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 51s 66ms/sample - loss: 0.3378 - acc: 0.9035\n",
      "95/95 [==============================] - 827s 9s/step - loss: 0.1113 - acc: 0.9569 - val_loss: 0.2997 - val_acc: 0.9035\n",
      "Epoch 14/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.1008 - acc: 0.9609 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 51s 67ms/sample - loss: 0.2999 - acc: 0.9113\n",
      "95/95 [==============================] - 826s 9s/step - loss: 0.1024 - acc: 0.9607 - val_loss: 0.2662 - val_acc: 0.9113\n",
      "Epoch 15/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.1024 - acc: 0.9624 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 51s 66ms/sample - loss: 0.3274 - acc: 0.9048\n",
      "95/95 [==============================] - 824s 9s/step - loss: 0.1035 - acc: 0.9618 - val_loss: 0.2919 - val_acc: 0.9048\n",
      "Epoch 16/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.1105 - acc: 0.9594 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 51s 66ms/sample - loss: 0.3057 - acc: 0.9100\n",
      "95/95 [==============================] - 822s 9s/step - loss: 0.1101 - acc: 0.9591 - val_loss: 0.2688 - val_acc: 0.9100\n",
      "Epoch 17/20\n",
      "94/95 [============================>.] - ETA: 8s - loss: 0.1019 - acc: 0.9657 Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 51s 66ms/sample - loss: 0.2971 - acc: 0.9113\n",
      "95/95 [==============================] - 824s 9s/step - loss: 0.1019 - acc: 0.9657 - val_loss: 0.2650 - val_acc: 0.9113\n",
      "Epoch 18/20\n",
      "94/95 [============================>.] - ETA: 10s - loss: 0.0896 - acc: 0.9710Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 73s 95ms/sample - loss: 0.3081 - acc: 0.9100\n",
      "95/95 [==============================] - 1097s 12s/step - loss: 0.0888 - acc: 0.9713 - val_loss: 0.2729 - val_acc: 0.9100\n",
      "Epoch 19/20\n",
      "94/95 [============================>.] - ETA: 14s - loss: 0.0884 - acc: 0.9678Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 72s 94ms/sample - loss: 0.3277 - acc: 0.9061\n",
      "95/95 [==============================] - 1448s 15s/step - loss: 0.0884 - acc: 0.9678 - val_loss: 0.2877 - val_acc: 0.9061\n",
      "Epoch 20/20\n",
      "94/95 [============================>.] - ETA: 25s - loss: 0.0902 - acc: 0.9683Epoch 1/20\n",
      "767/95 [==================================================================================================================================================================================================================================================] - 72s 94ms/sample - loss: 0.3319 - acc: 0.8996\n",
      "95/95 [==============================] - 2446s 26s/step - loss: 0.0905 - acc: 0.9680 - val_loss: 0.2935 - val_acc: 0.8996\n"
     ]
    }
   ],
   "source": [
    "print(\"[info] traininf head...\")\n",
    "#fitting our model to our variables\n",
    "h=model.fit(aug.flow(trainx,trainy,batch_size=bs),\n",
    "            steps_per_epoch=len(trainx) //bs,\n",
    "            validation_data=(testx,testy),\n",
    "            validation_steps=len(testx)//bs,\n",
    "            epochs=EPOCH\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]saving mask detector model.....\n"
     ]
    }
   ],
   "source": [
    "predIdxs = model.predict(testx, batch_size=bs)\n",
    "predIdxs = model.predict(testx, batch_size=bs)\n",
    "#print(classification_report(testy.argmax(axis=1),predIdxs,target_names=lb.classes_))\n",
    "\n",
    "print(\"[INFO]saving mask detector model.....\")\n",
    "#saving our model \n",
    "model.save(\"facemask_detector.model\",save_format=\"h5\")#among save model and h5 format h5 is lessweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
